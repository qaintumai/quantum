{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**qAIntum.ai**\n",
    "\n",
    "# **Quantum Neural Network (QNN) Classifier**\n",
    "\n",
    "This note is an example of using Photonic Analog (PA) QNN for binary classification. This is an application of the original work [\"Continuous Variable Quantum Neural Networks\"](https://arxiv.org/abs/1806.06871).\n",
    "\n",
    "Compared to Classical Neural Networks, PA QNNs have a reduced number of parameters to train and converge faster with fewer epochs. However, this is a quantum algorithm simulated on classical computers hence the training time for quantum circuits tend to be longer than classical models.\n",
    "\n",
    "The dataset used in this example can be found at https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database\n",
    "\n",
    "This file is organized in the following order:\n",
    "0. Install and import necessary packages\n",
    "1. Load and preprocess data (The data from Kaggle is saved as 'financial.csv'.)\n",
    "2. Data encoding\n",
    "3. QNN model\n",
    "   * QNN layer\n",
    "   * QNN circuit\n",
    "   * Model building\n",
    "4. Model training\n",
    "5. Evaluation\n",
    "\n",
    "For the open source repository, refer to https://github.com/qaintumai/quantum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Intall Packages**\n",
    "\n",
    "Ensure you have installed the packages detailed in requirements.txt before running the code below.\n",
    "\n",
    "[Pennylane](https://pennylane.ai/) is a Python based quantum machine learning library by Xanadu. An old version is necessary for this example: v0.29.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Load and Preprocess Data**\n",
    "\n",
    "Data: \n",
    "\n",
    "Data points: \n",
    "\n",
    "Number of features: \n",
    "\n",
    "Label: \n",
    "\n",
    "For the purpose of this experiment, we are using only ___ data points for training and __ for testing.\n",
    "\n",
    "data source: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset:  768\n",
      "Size of X tensor: torch.Size([768, 8]) and first element of X: tensor([  6.0000, 148.0000,  72.0000,  35.0000,   0.0000,  33.6000,   0.6270,\n",
      "         50.0000])\n",
      "Size of Y tensor: torch.Size([768, 1]) and first element of Y: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "#LOADING DATA\n",
    "import sys\n",
    "import os\n",
    "script_dir = os.getcwd()\n",
    "data_path = os.path.join(script_dir, '../examples/data/pima-indians-diabetes.csv')\n",
    "data_path = os.path.normpath(data_path)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = np.loadtxt(data_path, delimiter=',')\n",
    "\n",
    "print(\"length of dataset: \", len(dataset))\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "print(f\"Size of X tensor: {X.size()} and first element of X: {X[0]}\")\n",
    "print(f\"Size of Y tensor: {y.size()} and first element of Y: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Data Encoding**\n",
    "\n",
    "This step converts classical data into a quantum state by using the data entries as parameters of the quantum gates.\n",
    "\n",
    "The data encoding gates used are Squeezing, Rotation, Beamsplitter, Displacement Gate, and Kerr Gate. Other gates under \"CV operators\" in the Pennylane package can be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, num_wires):\n",
    "        \"\"\"\n",
    "        Encodes the input data into a quantum state to be operated on using a sequence of quantum gates.\n",
    "\n",
    "        Parameters:\n",
    "        x : input data (list or array-like)\n",
    "\n",
    "        The encoding process uses the following gates in sequence:\n",
    "        - Squeezing gates: 2*self.num_wires parameters\n",
    "        - Beamsplitter gates: 2(self.num_wires-1) parameters\n",
    "        - Rotation gates: self.num_wires parameters\n",
    "        - Displacement gates: 2*self.num_wires parameters\n",
    "        - Kerr gates: self.num_wires parameters\n",
    "          Total: 8*self.num_wires - 2 parameters\n",
    "\n",
    "        rounds: the number of iterations of the sequence needed to take in all the entries of the input data\n",
    "                num_features // (8 * self.num_wires - 2)\n",
    "                We are adding (8 * self.num_wires - 3) as a pad to run one extra round for the remainding data entries.\n",
    "        \"\"\"\n",
    "        num_features = len(x)\n",
    "        \n",
    "\n",
    "        # Calculate the number of rounds needed to process all features\n",
    "        rounds = (num_features + (8 * num_wires - 3)) // (8 * num_wires - 2)\n",
    "\n",
    "        for j in range(rounds):\n",
    "            start_idx = j * (8 * num_wires - 2)\n",
    "\n",
    "            # Squeezing gates\n",
    "            for i in range(num_wires):\n",
    "                # for each wire, the number of parameters are i*2\n",
    "                idx = start_idx + i * 2\n",
    "                if idx + 1 < num_features:\n",
    "                    qml.Squeezing(x[idx], x[idx + 1], wires=i)\n",
    "\n",
    "            # Beamsplitter gates\n",
    "            for i in range(num_wires - 1):\n",
    "                # start_index + Squeezing gates, and then i*2 parameters for each gate\n",
    "                idx = start_idx + num_wires * 2 + i * 2\n",
    "                if idx + 1 < num_features:\n",
    "                    qml.Beamsplitter(x[idx], x[idx + 1], wires=[i % num_wires, (i + 1) % num_wires])\n",
    "\n",
    "            # Rotation gates\n",
    "            for i in range(num_wires):\n",
    "                # start_index + Squeezing gates + Beamsplitters, and then i parameters for each gate\n",
    "                idx = start_idx + num_wires * 2 + (num_wires - 1) * 2 + i\n",
    "                if idx < num_features:\n",
    "                    qml.Rotation(x[idx], wires=i)\n",
    "\n",
    "            # Displacement gates\n",
    "            for i in range(num_wires):\n",
    "                # start_index + Squeezing gates + Beamsplitters + Rotation gates, and then i*2 parameters for each gate\n",
    "                idx = start_idx + num_wires * 2 + (num_wires - 1) * 2 + num_wires + i * 2\n",
    "                if idx + 1 < num_features:\n",
    "                    qml.Displacement(x[idx], x[idx + 1], wires=i)\n",
    "\n",
    "            # Kerr gates\n",
    "            for i in range(num_wires):\n",
    "                # start_index + Squeezing gates + Beamsplitters + Rotation gates + Displacement gates, and then i parameters for each gate\n",
    "                idx = start_idx + num_wires * 2 + (num_wires - 1) * 2 + num_wires + num_wires * 2 + i\n",
    "                if idx < num_features:\n",
    "                    qml.Kerr(x[idx], wires=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. QNN Model**\n",
    "To build a model, we need to\n",
    "* define a layer\n",
    "* build a circuit with the defined layer\n",
    "* build a model with the defined circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 QNN Layer**\n",
    "This in a faithful implementation of classical neural networks:\n",
    "* Weight matrix: Interferometer 1 + Squeezing + Interferometer 2\n",
    "* Bias addition: Displacement gate\n",
    "* Nonlinear activation function: Kerr gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply(v, num_wires):\n",
    "        \"\"\"\n",
    "        Applies the quantum neural network layer with the given parameters.\n",
    "\n",
    "        Parameters:\n",
    "        - v (list or array): List or array of parameters for the quantum gates.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        num_params = len(v)\n",
    "\n",
    "        # Interferometer 1\n",
    "        for i in range(num_wires - 1):\n",
    "            idx = i * 2\n",
    "            if idx + 1 < num_params:\n",
    "                theta = v[idx]\n",
    "                phi = v[idx + 1]\n",
    "                qml.Beamsplitter(theta, phi, wires=[i % num_wires, (i + 1) % num_wires])\n",
    "\n",
    "        for i in range(num_wires):\n",
    "            idx = (num_wires - 1) * 2 + i\n",
    "            if idx < num_params:\n",
    "                qml.Rotation(v[idx], wires=i)\n",
    "\n",
    "        # Squeezers\n",
    "        for i in range(num_wires):\n",
    "            idx = (num_wires - 1) * 2 + num_wires + i\n",
    "            if idx < num_params:\n",
    "                qml.Squeezing(v[idx], 0.0, wires=i)\n",
    "\n",
    "        # Interferometer 2\n",
    "        for i in range(num_wires - 1):\n",
    "            idx = (num_wires - 1) * 2 + num_wires + num_wires + i * 2\n",
    "            if idx + 1 < num_params:\n",
    "                theta = v[idx]\n",
    "                phi = v[idx + 1]\n",
    "                qml.Beamsplitter(theta, phi, wires=[i % num_wires, (i + 1) % num_wires])\n",
    "\n",
    "        for i in range(num_wires):\n",
    "            idx = (num_wires - 1) * 2 + num_wires + num_wires + (num_wires - 1) * 2 + i\n",
    "            if idx < num_params:\n",
    "                qml.Rotation(v[idx], wires=i)\n",
    "\n",
    "        # Bias addition\n",
    "        for i in range(num_wires):\n",
    "            idx = (num_wires - 1) * 2 + num_wires + num_wires + (num_wires - 1) * 2 + num_wires + i\n",
    "            if idx < num_params:\n",
    "                qml.Displacement(v[idx], 0.0, wires=i)\n",
    "\n",
    "        # Non-linear activation function\n",
    "        for i in range(num_wires):\n",
    "            idx = (num_wires - 1) * 2 + num_wires + num_wires + (num_wires - 1) * 2 + num_wires + num_wires + i\n",
    "            if idx < num_params:\n",
    "                qml.Kerr(v[idx], wires=i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight Initializer**\n",
    "\n",
    "Randomly initialized values are used as initial parameters of the QNN circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layers, num_wires, active_sd=0.0001, passive_sd=0.1):\n",
    "    \"\"\"\n",
    "    This is a weight vector initializer.\n",
    "    Input: number of layers, number of wires\n",
    "    Output: concatenated weight vector\n",
    "    \"\"\"\n",
    "    M = (num_wires - 1) * 2 + num_wires  # Number of interferometer parameters\n",
    "\n",
    "    int1_weights = np.random.normal(size=[layers, M], scale=passive_sd) #beamsplitters and rotations\n",
    "    s_weights = np.random.normal(size=[layers, num_wires], scale=active_sd) #squeezers\n",
    "    int2_weights = np.random.normal(size=[layers, M], scale=passive_sd) #beamsplitters and rotations\n",
    "    dr_weights = np.random.normal(size=[layers, num_wires], scale=active_sd) #displacement\n",
    "    k_weights = np.random.normal(size=[layers, num_wires], scale=active_sd) #Kerr\n",
    "\n",
    "    weights = np.concatenate(\n",
    "        [int1_weights, s_weights, int2_weights, dr_weights, k_weights], axis=1)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 QNN Circuit**\n",
    "\n",
    "For building a PA circuit as opposed to a qubit-based circuit, we have to choose \"strawberryfields.fock\" as device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_wires = 8\n",
    "num_basis = 2\n",
    "\n",
    "# select a device\n",
    "dev = qml.device(\"strawberryfields.fock\", wires=num_wires, cutoff_dim=num_basis)\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_nn(inputs, var):\n",
    "    \"\"\"\n",
    "    This is a quantum circuit composed of a data encoding circuit and a QNN circuit.\n",
    "    Input: classical data (inputs), quantum parameters (var)\n",
    "    Output: quantum state, converted to classical data after measurement\n",
    "    \"\"\"\n",
    "    # convert classical inputs into quantum states\n",
    "    encode(inputs, num_wires)\n",
    "    print(\"Encoded data\")\n",
    "\n",
    "    # iterative quantum layers\n",
    "    for v in var:\n",
    "        apply(v, num_wires)\n",
    "\n",
    "    # measure the resulting state and return\n",
    "    return qml.expval(qml.X(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "\n",
    "def get_model(num_wires, num_layers):\n",
    "    \"\"\"\n",
    "    This is a model building function.\n",
    "    Input: number of modes, number of layers\n",
    "    Output: PyTorch model\n",
    "    \"\"\"\n",
    "    weights = init_weights(num_layers, num_wires)\n",
    "    shape_tup = weights.shape\n",
    "    weight_shapes = {'var': shape_tup}\n",
    "    qlayer = qml.qnn.TorchLayer(quantum_nn, weight_shapes)\n",
    "    model = torch.nn.Sequential(qlayer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x12b788e40>\n",
      "Sequential(\n",
      "  (0): <Quantum Torch Layer: func=quantum_nn>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_model(num_wires, num_layers)\n",
    "print(model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================\n",
      "+++++++++++++++Batch number: 0 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 2 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 4 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 6 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 8 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 100.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 10 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 12 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 14 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 100.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 16 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 100.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 18 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 50.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 20 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 0.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 22 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 100.0\n",
      "====================================================================================\n",
      "+++++++++++++++Batch number: 24 in Epoch 0++++++++++++++++++++++++++++++++++\n",
      "====================================================================================\n",
      "Size of Xbatch: torch.Size([2, 8])\n",
      "Encoded data\n",
      "Encoded data\n",
      "Size of y_pred: torch.Size([2, 1]), Values: tensor([[0.],\n",
      "        [0.]], grad_fn=<ViewBackward0>)\n",
      "loss = 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, latest loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane/interfaces/torch.py:177\u001b[0m, in \u001b[0;36mExecuteTapes.backward\u001b[0;34m(ctx, *dy)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[39mwith\u001b[39;00m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mUnwrap(\u001b[39m*\u001b[39mctx\u001b[39m.\u001b[39mtapes):\n\u001b[1;32m    169\u001b[0m             vjp_tapes, processing_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mgradients\u001b[39m.\u001b[39mbatch_vjp(\n\u001b[1;32m    170\u001b[0m                 ctx\u001b[39m.\u001b[39mtapes,\n\u001b[1;32m    171\u001b[0m                 dy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 gradient_kwargs\u001b[39m=\u001b[39mctx\u001b[39m.\u001b[39mgradient_kwargs,\n\u001b[1;32m    175\u001b[0m             )\n\u001b[0;32m--> 177\u001b[0m             vjps \u001b[39m=\u001b[39m processing_fn(ctx\u001b[39m.\u001b[39;49mexecute_fn(vjp_tapes)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# Gradient function is not a gradient transform\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39m# (e.g., it might be a device method).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39m# so we cannot support higher-order derivatives.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[39mwith\u001b[39;00m qml\u001b[39m.\u001b[39mtape\u001b[39m.\u001b[39mUnwrap(\u001b[39m*\u001b[39mctx\u001b[39m.\u001b[39mtapes):\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane/interfaces/execution.py:205\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[39mreturn\u001b[39;00m (res, []) \u001b[39mif\u001b[39;00m return_tuple \u001b[39melse\u001b[39;00m res\n\u001b[1;32m    203\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[39m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     res \u001b[39m=\u001b[39m fn(execution_tapes\u001b[39m.\u001b[39;49mvalues(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m final_res \u001b[39m=\u001b[39m []\n\u001b[1;32m    209\u001b[0m \u001b[39mfor\u001b[39;00m i, tape \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tapes):\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane/interfaces/execution.py:131\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[0;34m(tapes, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(tapes: Sequence[QuantumTape], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     tapes \u001b[39m=\u001b[39m [expand_fn(tape) \u001b[39mfor\u001b[39;00m tape \u001b[39min\u001b[39;00m tapes]\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn(tapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane/_device.py:530\u001b[0m, in \u001b[0;36mDevice.batch_execute\u001b[0;34m(self, circuits)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39mfor\u001b[39;00m circuit \u001b[39min\u001b[39;00m circuits:\n\u001b[1;32m    526\u001b[0m     \u001b[39m# we need to reset the device here, else it will\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[39m# not start the next computation in the zero state\u001b[39;00m\n\u001b[1;32m    528\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 530\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(circuit\u001b[39m.\u001b[39;49moperations, circuit\u001b[39m.\u001b[39;49mobservables)\n\u001b[1;32m    531\u001b[0m     results\u001b[39m.\u001b[39mappend(res)\n\u001b[1;32m    533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mactive:\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane/_device.py:459\u001b[0m, in \u001b[0;36mDevice.execute\u001b[0;34m(self, queue, observables, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply(operation\u001b[39m.\u001b[39mname, operation\u001b[39m.\u001b[39mwires, operation\u001b[39m.\u001b[39mparameters)\n\u001b[1;32m    457\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_apply()\n\u001b[0;32m--> 459\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_measure()\n\u001b[1;32m    461\u001b[0m \u001b[39mfor\u001b[39;00m obs \u001b[39min\u001b[39;00m observables:\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, Tensor):\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/pennylane_sf/fock.py:131\u001b[0m, in \u001b[0;36mStrawberryFieldsFock.pre_measure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_measure\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meng \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mEngine(\u001b[39m\"\u001b[39m\u001b[39mfock\u001b[39m\u001b[39m\"\u001b[39m, backend_options\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mcutoff_dim\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcutoff})\n\u001b[0;32m--> 131\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meng\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprog)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mstate\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39msamples\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/strawberryfields/engine.py:570\u001b[0m, in \u001b[0;36mLocalEngine.run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[39mif\u001b[39;00m c\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mmeasurement_deps \u001b[39mand\u001b[39;00m eng_run_options[\u001b[39m\"\u001b[39m\u001b[39mshots\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    566\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFeed-forwarding of measurements cannot be used together with multiple shots.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             )\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_run(\n\u001b[1;32m    571\u001b[0m     program_lst, args\u001b[39m=\u001b[39;49margs, compile_options\u001b[39m=\u001b[39;49mcompile_options, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49meng_run_options\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/strawberryfields/engine.py:306\u001b[0m, in \u001b[0;36mBaseEngine._run\u001b[0;34m(self, program, args, compile_options, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m p\u001b[39m.\u001b[39mbind_params(args)\n\u001b[1;32m    304\u001b[0m p\u001b[39m.\u001b[39mlock()\n\u001b[0;32m--> 306\u001b[0m _, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_program(p, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_progs\u001b[39m.\u001b[39mappend(p)\n\u001b[1;32m    309\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p, TDMProgram) \u001b[39mand\u001b[39;00m received_rolled:\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/strawberryfields/engine.py:430\u001b[0m, in \u001b[0;36mLocalEngine._run_program\u001b[0;34m(self, prog, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mfor\u001b[39;00m cmd \u001b[39min\u001b[39;00m prog\u001b[39m.\u001b[39mcircuit:\n\u001b[1;32m    428\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m         \u001b[39m# try to apply it to the backend and, if op is a measurement, store it in values\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m         val \u001b[39m=\u001b[39m cmd\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mapply(cmd\u001b[39m.\u001b[39;49mreg, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackend, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    431\u001b[0m         \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m             \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(cmd\u001b[39m.\u001b[39mreg):\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/strawberryfields/ops.py:497\u001b[0m, in \u001b[0;36mGate.apply\u001b[0;34m(self, reg, backend, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp[\u001b[39m0\u001b[39m]\n\u001b[1;32m    495\u001b[0m \u001b[39m# if z represents a batch of parameters, then all of these\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m# must be zero to skip calling backend\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39;49mall(z \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m):\n\u001b[1;32m    498\u001b[0m     \u001b[39m# identity, no need to apply\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdagger:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2489\u001b[0m, in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[1;32m   2406\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_all_dispatcher)\n\u001b[1;32m   2407\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39m_NoValue):\n\u001b[1;32m   2408\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m \u001b[39m    Test whether all array elements along a given axis evaluate to True.\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2487\u001b[0m \n\u001b[1;32m   2488\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2489\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49mlogical_and, \u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, \u001b[39mNone\u001b[39;49;00m, out,\n\u001b[1;32m   2490\u001b[0m                           keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/Desktop/QuAIntum/quantum/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#TRAINING MODEL\n",
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 1\n",
    "batch_size = 2\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        print(\"====================================================================================\")\n",
    "        print(f\"+++++++++++++++Batch number: {i} in Epoch {epoch}++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"====================================================================================\")\n",
    "\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        print(f\"Size of Xbatch: {Xbatch.size()}\")\n",
    "        y_pred = model(Xbatch).reshape(-1, 1)  \n",
    "        print(f\"Size of y_pred: {y_pred.size()}, Values: {y_pred}\")\n",
    "\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        print(f\"loss = {loss}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATING\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "    print(f\"Size of y_pred (evaluation): {y_pred.size()}, Values: {y_pred}\")\n",
    "\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eed3d6a5ebd92ef48ec685a6188b4208468e30da06e40b2114167f15b521e1ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
